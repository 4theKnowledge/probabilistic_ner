{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models and LSTM Networks - Probablistic Model with Pyro and Penn Treebank\n",
    "Sequence Tagger: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html<br>\n",
    "Bayesian NN: https://github.com/paraschopra/bayesian-neural-network-mnist/blob/master/bnn.ipynb<br>\n",
    "Penn Treebank: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/williamFalcon/f27c7b90e34b4ba88ced042d9ef33edd <br>\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penn tree bank\n",
    "sentences = treebank.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 5000\n",
    "sentences = sentences[:samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sequence(seq):\n",
    "    \"\"\"\n",
    "    Formats penn treebank POS format into tuple ([tokens], [POS])\n",
    "    \"\"\"\n",
    "    tokens = [x[0] for x in seq]\n",
    "    tags = [x[1] for x in seq]\n",
    "    return (tokens, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [format_sequence(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_vocab(sentences):\n",
    "    \"\"\"Builds vocab based on input data\"\"\"\n",
    "    vocab = dict()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence[0]:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab of input data (this will likely be a subset of any word embedding array)\n",
    "data_vocab = data_vocab(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Encodes sentence tokens as ids from word_to_ix dictionary\"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "split_ratio = 0.80\n",
    "training_data = sentences[:int(len(sentences)*split_ratio)]\n",
    "test_data = sentences[len(training_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset Size: {len(sentences)} | Training Set Size: {len(training_data)} | Test Set Size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"John likes the blue house at the end of the street\".split(), [\"NN\", \"V\", \"DET\", \"ADJ\", \"NN\", \"PREP\", \"DET\", \"NN\", \"PREP\", \"DET\", \"NN\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in sentences:   # training_data\n",
    "#     print(sent, tags)\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "# print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tag-index lookups\n",
    "tag_to_ix = {}\n",
    "for _, tags in sentences:\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "ix_to_tag = {v:k for k, v in tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ix_to_tag = {0:\"DET\", 1:\"NN\", 2:\"V\", 3: \"ADJ\", 4: \"PREP\"}\n",
    "tag_to_ix = {\"DET\":0, \"NN\":1, \"V\":2, \"ADJ\": 3, \"PREP\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Word dictionary size: {len(word_to_ix)}')\n",
    "print(f'Tag dictionary size: {len(tag_to_ix)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, batch_size=32, pretrained_embeddings=None):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.word_embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "            self.word_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)    # , batch_first=True\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.out(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the NN model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(File,'r')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_embeddings = './data/embeddings/glove.6B.300d.txt'\n",
    "path_to_trimmed_embeddings = './data/embeddings/trimmed_emb.npz'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gloveModel = loadGloveModel(path_to_embeddings)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "emb_vocab = set(gloveModel.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://github.com/Michael-Stewart-Webdev/pytorch-lexnorm/blob/master/build_data.py\n",
    "# https://leakyrelu.com/2019/10/18/using-glove-word-embeddings-with-seq2seq-encoder-decoder-in-pytorch/\n",
    "def export_emb_vectors(data_vocab, emb_filename, trimmed_emb_filename, dim):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings = np.zeros([len(data_vocab), dim])\n",
    "\n",
    "    with codecs.open(emb_filename, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if word in data_vocab:\n",
    "                word_idx = data_vocab[word]\n",
    "                clear_output(wait=True)\n",
    "                print(word, word_idx)\n",
    "                \n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "    \n",
    "    np.savez_compressed(trimmed_emb_filename, embeddings=embeddings)\n",
    "    print('Saved trimmed embeddings to disk')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "export_emb_vectors(data_vocab, path_to_embeddings, path_to_trimmed_embeddings, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trimmed embeddings from disk\n",
    "pretrained_embeddings = np.load(path_to_trimmed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check embedding shape\n",
    "pretrained_embeddings['embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300   # Glove 300\n",
    "HIDDEN_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_net = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix), pretrained_embeddings=pretrained_embeddings['embeddings'])\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train standard NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        lstm_net.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = lstm_net(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch} - Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for deterministic nn inference\n",
    "def tag_score_to_tag_name(tag_score, ix_to_tag):\n",
    "    \"\"\"Converts tag score to tag names\"\"\"\n",
    "    return ix_to_tag.get(torch.argmax(tag_score).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single test example\n",
    "test_data_sm = test_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(test_data_sm[0][0], word_to_ix)\n",
    "    tag_scores = lstm_net(inputs)\n",
    "    \n",
    "#     print(f'Tag Scores:\\n{tag_scores}\\n')\n",
    "    print(f'{\"Token\":<20} {\"Pred\":<10} {\"Actual\":<10}')\n",
    "    print(f'{\"-----\":<20} {\"----\":<10} {\"------\":<10}')\n",
    "    for i, token in enumerate(training_data[0][0]):\n",
    "        print(f'{token:<20} {tag_score_to_tag_name(tag_scores[i], ix_to_tag):<10} {test_data_sm[0][1][i]:<10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Probabilistic Pyro Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:<br>\n",
    "- https://forum.pyro.ai/t/bayesian-rnn-nan-loss-issue/254"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# init log softmax for application to n-dim tensors\n",
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loc = mean, Scale = standard deviation\n",
    "- mu = 0, sigma = 1 -> Unit Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:<br>\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See fix for issues relating to lognorm: https://github.com/pyro-ppl/pyro/issues/1409 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise lstm_net (in case it was being used previously)\n",
    "lstm_net = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix), pretrained_embeddings=pretrained_embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input, target):\n",
    "    \n",
    "    # Embeddings (current model uses pre-trained embeddings)\n",
    "#     word_embeddings_w_prior = Normal(loc=torch.zeros_like(lstm_net.word_embeddings.weight),\n",
    "#                                      scale=torch.ones_like(lstm_net.word_embeddings.weight)).independent(2)\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_w_ih_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.weight_ih_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.weight_ih_l0)).independent(2)\n",
    "    lstm_w_hh_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.weight_hh_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.weight_hh_l0)).independent(2)\n",
    "    lstm_b_ih_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.bias_ih_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.bias_ih_l0)).independent(1)\n",
    "    lstm_b_hh_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.bias_hh_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.bias_hh_l0)).independent(1)\n",
    "    \n",
    "    # Output\n",
    "    out_w_prior = Normal(loc=torch.zeros_like(lstm_net.out.weight),\n",
    "                          scale=torch.ones_like(lstm_net.out.weight)).independent(2)\n",
    "    out_b_prior = Normal(loc=torch.zeros_like(lstm_net.out.bias),\n",
    "                          scale=torch.ones_like(lstm_net.out.bias)).independent(1)\n",
    "    \n",
    "    \n",
    "    priors = {'lstm.weight_ih_l0': lstm_w_ih_l0_prior,\n",
    "              'lstm.weight_hh_l0': lstm_w_hh_l0_prior,\n",
    "              'lstm.bias_ih_l0': lstm_b_ih_l0_prior,\n",
    "              'lstm.bias_hh_l0': lstm_b_hh_l0_prior,\n",
    "              'out.weight': out_w_prior,\n",
    "              'out.bias': out_b_prior}   # 'word_embeddings.weight': word_embeddings_w_prior,\n",
    "    \n",
    "    # Lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", lstm_net, priors)\n",
    "    \n",
    "    # Sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "#     lhat = log_softmax(lifted_reg_model(input))\n",
    "    output = lifted_reg_model(input)\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=output).independent(1), obs=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(input, target):\n",
    "    \n",
    "    # Embedding layer weight distribution priors (current model uses pre-trained embeddings)\n",
    "#     word_embeddings_w_mu = torch.randn_like(lstm_net.word_embeddings.weight)\n",
    "#     word_embeddings_w_sigma = torch.randn_like(lstm_net.word_embeddings.weight)\n",
    "#     word_embeddings_w_mu_param = pyro.param(\"word_embeddings_w_mu\", word_embeddings_w_mu)\n",
    "#     word_embeddings_w_sigma_param = softplus(pyro.param(\"word_embeddings_w_sigma\", word_embeddings_w_sigma))\n",
    "#     word_embeddings_w_prior = Normal(loc=word_embeddings_w_mu_param, scale=word_embeddings_w_sigma_param).independent(2)\n",
    "    \n",
    "    # LSTM layer weight distribution priors\n",
    "    lstm_w_ih_l0_mu = torch.randn_like(lstm_net.lstm.weight_ih_l0)\n",
    "    lstm_w_ih_l0_sigma = torch.randn_like(lstm_net.lstm.weight_ih_l0)\n",
    "    lstm_w_ih_l0_mu_param = pyro.param(\"lstm_w_ih_l0_mu\", lstm_w_ih_l0_mu)\n",
    "    lstm_w_ih_l0_sigma_param = softplus(pyro.param(\"lstm_w_ih_l0_sigma\", lstm_w_ih_l0_sigma))\n",
    "    lstm_w_ih_l0_prior = Normal(loc=lstm_w_ih_l0_mu_param, scale=lstm_w_ih_l0_sigma_param).independent(2)\n",
    "    \n",
    "    lstm_w_hh_l0_mu = torch.randn_like(lstm_net.lstm.weight_hh_l0)\n",
    "    lstm_w_hh_l0_sigma = torch.randn_like(lstm_net.lstm.weight_hh_l0)\n",
    "    lstm_w_hh_l0_mu_param = pyro.param(\"lstm_w_hh_l0_mu\", lstm_w_hh_l0_mu)\n",
    "    lstm_w_hh_l0_sigma_param = softplus(pyro.param(\"lstm_w_hh_l0_sigma\", lstm_w_hh_l0_sigma))\n",
    "    lstm_w_hh_l0_prior = Normal(loc=lstm_w_hh_l0_mu_param, scale=lstm_w_hh_l0_sigma_param).independent(2)\n",
    "    \n",
    "    # LSTM layer bias distribution priors\n",
    "    lstm_b_ih_l0_mu = torch.randn_like(lstm_net.lstm.bias_ih_l0)\n",
    "    lstm_b_ih_l0_sigma = torch.randn_like(lstm_net.lstm.bias_ih_l0)\n",
    "    lstm_b_ih_l0_mu_param = pyro.param(\"lstm_b_ih_l0_mu\", lstm_b_ih_l0_mu)\n",
    "    lstm_b_ih_l0_sigma_param = softplus(pyro.param(\"lstm_b_ih_l0_sigma\", lstm_b_ih_l0_sigma))\n",
    "    lstm_b_ih_l0_prior = Normal(loc=lstm_b_ih_l0_mu_param, scale=lstm_b_ih_l0_sigma_param).independent(1)\n",
    "    \n",
    "    lstm_b_hh_l0_mu = torch.randn_like(lstm_net.lstm.bias_hh_l0)\n",
    "    lstm_b_hh_l0_sigma = torch.randn_like(lstm_net.lstm.bias_hh_l0)\n",
    "    lstm_b_hh_l0_mu_param = pyro.param(\"lstm_b_hh_l0_mu\", lstm_b_hh_l0_mu)\n",
    "    lstm_b_hh_l0_sigma_param = softplus(pyro.param(\"lstm_b_hh_l0_sigma\", lstm_b_hh_l0_sigma))\n",
    "    lstm_b_hh_l0_prior = Normal(loc=lstm_b_hh_l0_mu_param, scale=lstm_b_hh_l0_sigma_param).independent(1)\n",
    "    \n",
    "    # Output layer weight distribution priors\n",
    "    out_w_mu = torch.randn_like(lstm_net.out.weight)\n",
    "    out_w_sigma = torch.randn_like(lstm_net.out.weight)\n",
    "    out_w_mu_param = pyro.param(\"out_w_mu\", out_w_mu)\n",
    "    out_w_sigma_param = softplus(pyro.param(\"out_w_sigma\", out_w_sigma))\n",
    "    out_w_prior = Normal(loc=out_w_mu_param, scale=out_w_sigma_param).independent(2)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    out_b_mu = torch.randn_like(lstm_net.out.bias)\n",
    "    out_b_sigma = torch.randn_like(lstm_net.out.bias)\n",
    "    out_b_mu_param = pyro.param(\"out_b_mu\", out_b_mu)\n",
    "    out_b_sigma_param = softplus(pyro.param(\"out_b_sigma\", out_b_sigma))\n",
    "    out_b_prior = Normal(loc=out_b_mu_param, scale=out_b_sigma_param).independent(1)\n",
    "    \n",
    "    priors = {'lstm.weight_ih_l0': lstm_w_ih_l0_prior,\n",
    "              'lstm.weight_hh_l0': lstm_w_hh_l0_prior,\n",
    "              'lstm.bias_ih_l0': lstm_b_ih_l0_prior,\n",
    "              'lstm.bias_hh_l0': lstm_b_hh_l0_prior,\n",
    "              'out.weight': out_w_prior,\n",
    "              'out.bias': out_b_prior}    # 'word_embeddings.weight': word_embeddings_w_prior,\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", lstm_net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = SVI(model, guide, Adam({\"lr\": 0.01}), loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update to use batches; atm its single sample... very slow\n",
    "\n",
    "num_iterations = 10\n",
    "loss = 0\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for sentence, tags in training_data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        # Calculate loss and take gradient step\n",
    "        loss += inference.step(sentence_in, targets)\n",
    "    \n",
    "    total_epoch_loss_train = loss / len(training_data)\n",
    "    \n",
    "    if j % 1 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f'Epoch {j} - Loss {total_epoch_loss_train:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, num_samples):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise set of probablistic models for inference\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    \n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    print(f'\\nyhats:\\n{yhats}')\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    print(f'\\nMean:\\n{mean}')\n",
    "    return np.argmax(mean.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def tag_score_to_tag_name(tag_score, ix_to_tag):\n",
    "    \"\"\"\n",
    "    Converts tag score to tag names\n",
    "    \"\"\"\n",
    "    if type(tag_score).__module__ == np.__name__:\n",
    "        return ix_to_tag.get(np.argmax(tag_score))\n",
    "    if torch.is_tensor(tag_score):\n",
    "        return ix_to_tag.get(torch.argmax(tag_score).item())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_data = [\n",
    "    (\"The dog likes the blue apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"ADJ\", \"NN\"]),\n",
    "    (\"John likes that blue book\".split(), [\"NN\", \"V\", \"DET\", \"ADJ\", \"NN\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sm = test_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "num_samples = 10\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(test_data_sm):\n",
    "    sentence, tags = data\n",
    "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "    print(sentence, tags)\n",
    "    \n",
    "    # Convert tags into their indexes in tag dictionary\n",
    "    tag_indices = np.array([tag_to_ix.get(tag) for tag in tags])\n",
    "    \n",
    "    predicted = predict(sentence_in, num_samples)\n",
    "    total += len(tags)\n",
    "    correct += (predicted == tag_indices).sum()\n",
    "    \n",
    "    for i, token in enumerate(sentence):\n",
    "        print(f'{token:<10} {ix_to_tag.get(predicted[i])}')\n",
    "    print('\\n')\n",
    "\n",
    "print(f'Accuracy: {correct/total * 100:0.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
