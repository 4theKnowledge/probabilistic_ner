{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models and LSTM Networks - Probablistic Model with Pyro and Penn Treebank\n",
    "Sequence Tagger: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html<br>\n",
    "Bayesian NN: https://github.com/paraschopra/bayesian-neural-network-mnist/blob/master/bnn.ipynb<br>\n",
    "Penn Treebank: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/tyler/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tyler/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f088617be10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penn tree bank\n",
    "sentences = treebank.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 200\n",
    "sentences = sentences[:samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sequence(seq):\n",
    "    \"\"\"\n",
    "    Formats penn treebank POS format into tuple ([tokens], [POS])\n",
    "    \"\"\"\n",
    "    tokens = [x[0] for x in seq]\n",
    "    tags = [x[1] for x in seq]\n",
    "    return (tokens, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [format_sequence(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Encodes sentence tokens as ids from word_to_ix dictionary\"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "split_ratio = 0.80\n",
    "training_data = sentences[:int(len(sentences)*split_ratio)]\n",
    "test_data = sentences[len(training_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 200 | Training Set Size: 160 | Test Set Size: 40\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset Size: {len(sentences)} | Training Set Size: {len(training_data)} | Test Set Size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"John likes the blue house at the end of the street\".split(), [\"NN\", \"V\", \"DET\", \"ADJ\", \"NN\", \"PREP\", \"DET\", \"NN\", \"PREP\", \"DET\", \"NN\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in sentences:   # training_data\n",
    "#     print(sent, tags)\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "# print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tag-index lookups\n",
    "tag_to_ix = {}\n",
    "for _, tags in sentences:\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "ix_to_tag = {v:k for k, v in tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ix_to_tag = {0:\"DET\", 1:\"NN\", 2:\"V\", 3: \"ADJ\", 4: \"PREP\"}\n",
    "tag_to_ix = {\"DET\":0, \"NN\":1, \"V\":2, \"ADJ\": 3, \"PREP\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word dictionary size: 1660\n",
      "Tag dictionary size: 12\n"
     ]
    }
   ],
   "source": [
    "print(f'Word dictionary size: {len(word_to_ix)}')\n",
    "print(f'Tag dictionary size: {len(tag_to_ix)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)    # , batch_first=True\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.out(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_net = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(1660, 32)\n",
      "  (lstm): LSTM(32, 32)\n",
      "  (out): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lstm_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Pyro model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:<br>\n",
    "- https://forum.pyro.ai/t/bayesian-rnn-nan-loss-issue/254"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# init log softmax for application to n-dim tensors\n",
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loc = mean, Scale = standard deviation\n",
    "- mu = 0, sigma = 1 -> Unit Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:<br>\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input, target):\n",
    "    \n",
    "    # Embeddings\n",
    "    word_embeddings_w_prior = Normal(loc=torch.zeros_like(lstm_net.word_embeddings.weight),\n",
    "                                     scale=torch.ones_like(lstm_net.word_embeddings.weight))\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_w_ih_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.weight_ih_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.weight_ih_l0))\n",
    "    lstm_w_hh_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.weight_hh_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.weight_hh_l0))\n",
    "    lstm_b_ih_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.bias_ih_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.bias_ih_l0))\n",
    "    lstm_b_hh_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.bias_hh_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.bias_hh_l0))\n",
    "    \n",
    "    # Output\n",
    "    out_w_prior = Normal(loc=torch.zeros_like(lstm_net.out.weight),\n",
    "                          scale=torch.ones_like(lstm_net.out.weight))\n",
    "    out_b_prior = Normal(loc=torch.zeros_like(lstm_net.out.bias),\n",
    "                          scale=torch.ones_like(lstm_net.out.bias))\n",
    "    \n",
    "    \n",
    "    priors = {'word_embeddings.weight': word_embeddings_w_prior,\n",
    "              'lstm.weight_ih_l0': lstm_w_ih_l0_prior,\n",
    "              'lstm.weight_hh_l0': lstm_w_hh_l0_prior,\n",
    "              'lstm.bias_ih_l0': lstm_b_ih_l0_prior,\n",
    "              'lstm.bias_hh_l0': lstm_b_hh_l0_prior,\n",
    "              'out.weight': out_w_prior,\n",
    "              'out.bias': out_b_prior}\n",
    "    \n",
    "    # Lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", lstm_net, priors)\n",
    "    \n",
    "    # Sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "#     lhat = log_softmax(lifted_reg_model(input))\n",
    "    output = lifted_reg_model(input)\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=output), obs=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(input, target):\n",
    "    \n",
    "    # Embedding layer weight distribution priors\n",
    "    word_embeddings_w_mu = torch.randn_like(lstm_net.word_embeddings.weight)\n",
    "    word_embeddings_w_sigma = torch.randn_like(lstm_net.word_embeddings.weight)\n",
    "    word_embeddings_w_mu_param = pyro.param(\"word_embeddings_w_mu\", word_embeddings_w_mu)\n",
    "    word_embeddings_w_sigma_param = softplus(pyro.param(\"word_embeddings_w_sigma\", word_embeddings_w_sigma))\n",
    "    word_embeddings_w_prior = Normal(loc=word_embeddings_w_mu_param, scale=word_embeddings_w_sigma_param)\n",
    "    \n",
    "    # LSTM layer weight distribution priors\n",
    "    lstm_w_ih_l0_mu = torch.randn_like(lstm_net.lstm.weight_ih_l0)\n",
    "    lstm_w_ih_l0_sigma = torch.randn_like(lstm_net.lstm.weight_ih_l0)\n",
    "    lstm_w_ih_l0_mu_param = pyro.param(\"lstm_w_ih_l0_mu\", lstm_w_ih_l0_mu)\n",
    "    lstm_w_ih_l0_sigma_param = softplus(pyro.param(\"lstm_w_ih_l0_sigma\", lstm_w_ih_l0_sigma))\n",
    "    lstm_w_ih_l0_prior = Normal(loc=lstm_w_ih_l0_mu_param, scale=lstm_w_ih_l0_sigma_param)\n",
    "    \n",
    "    lstm_w_hh_l0_mu = torch.randn_like(lstm_net.lstm.weight_hh_l0)\n",
    "    lstm_w_hh_l0_sigma = torch.randn_like(lstm_net.lstm.weight_hh_l0)\n",
    "    lstm_w_hh_l0_mu_param = pyro.param(\"lstm_w_hh_l0_mu\", lstm_w_hh_l0_mu)\n",
    "    lstm_w_hh_l0_sigma_param = softplus(pyro.param(\"lstm_w_hh_l0_sigma\", lstm_w_hh_l0_sigma))\n",
    "    lstm_w_hh_l0_prior = Normal(loc=lstm_w_hh_l0_mu_param, scale=lstm_w_hh_l0_sigma_param)\n",
    "    \n",
    "    # LSTM layer bias distribution priors\n",
    "    lstm_b_ih_l0_mu = torch.randn_like(lstm_net.lstm.bias_ih_l0)\n",
    "    lstm_b_ih_l0_sigma = torch.randn_like(lstm_net.lstm.bias_ih_l0)\n",
    "    lstm_b_ih_l0_mu_param = pyro.param(\"lstm_b_ih_l0_mu\", lstm_b_ih_l0_mu)\n",
    "    lstm_b_ih_l0_sigma_param = softplus(pyro.param(\"lstm_b_ih_l0_sigma\", lstm_b_ih_l0_sigma))\n",
    "    lstm_b_ih_l0_prior = Normal(loc=lstm_b_ih_l0_mu_param, scale=lstm_b_ih_l0_sigma_param)\n",
    "    \n",
    "    lstm_b_hh_l0_mu = torch.randn_like(lstm_net.lstm.bias_hh_l0)\n",
    "    lstm_b_hh_l0_sigma = torch.randn_like(lstm_net.lstm.bias_hh_l0)\n",
    "    lstm_b_hh_l0_mu_param = pyro.param(\"lstm_b_hh_l0_mu\", lstm_b_hh_l0_mu)\n",
    "    lstm_b_hh_l0_sigma_param = softplus(pyro.param(\"lstm_b_hh_l0_sigma\", lstm_b_hh_l0_sigma))\n",
    "    lstm_b_hh_l0_prior = Normal(loc=lstm_b_hh_l0_mu_param, scale=lstm_b_hh_l0_sigma_param)\n",
    "    \n",
    "    # Output layer weight distribution priors\n",
    "    out_w_mu = torch.randn_like(lstm_net.out.weight)\n",
    "    out_w_sigma = torch.randn_like(lstm_net.out.weight)\n",
    "    out_w_mu_param = pyro.param(\"out_w_mu\", out_w_mu)\n",
    "    out_w_sigma_param = softplus(pyro.param(\"out_w_sigma\", out_w_sigma))\n",
    "    out_w_prior = Normal(loc=out_w_mu_param, scale=out_w_sigma_param)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    out_b_mu = torch.randn_like(lstm_net.out.bias)\n",
    "    out_b_sigma = torch.randn_like(lstm_net.out.bias)\n",
    "    out_b_mu_param = pyro.param(\"out_b_mu\", out_b_mu)\n",
    "    out_b_sigma_param = softplus(pyro.param(\"out_b_sigma\", out_b_sigma))\n",
    "    out_b_prior = Normal(loc=out_b_mu_param, scale=out_b_sigma_param)\n",
    "    \n",
    "    priors = {'word_embeddings.weight': word_embeddings_w_prior,\n",
    "              'lstm.weight_ih_l0': lstm_w_ih_l0_prior,\n",
    "              'lstm.weight_hh_l0': lstm_w_hh_l0_prior,\n",
    "              'lstm.bias_ih_l0': lstm_b_ih_l0_prior,\n",
    "              'lstm.bias_hh_l0': lstm_b_hh_l0_prior,\n",
    "              'out.weight': out_w_prior,\n",
    "              'out.bias': out_b_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", lstm_net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = SVI(model, guide, Adam({\"lr\": 0.01}), loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 - Loss 363.3785\n"
     ]
    }
   ],
   "source": [
    "# TODO: update to use batches; atm its single sample... \n",
    "\n",
    "num_iterations = 100\n",
    "loss = 0\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for sentence, tags in training_data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        # Calculate loss and take gradient step\n",
    "        loss += inference.step(sentence_in, targets)\n",
    "    \n",
    "    total_epoch_loss_train = loss / len(training_data)\n",
    "    \n",
    "    if j % 1 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f'Epoch {j} - Loss {total_epoch_loss_train:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, num_samples):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise set of probablistic models for inference\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    \n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "#     print(f'\\nyhats:\\n{yhats[0]}')\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    print(f'\\nMean:\\n{mean}')\n",
    "    return np.argmax(mean.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def tag_score_to_tag_name(tag_score, ix_to_tag):\n",
    "    \"\"\"\n",
    "    Converts tag score to tag names\n",
    "    \"\"\"\n",
    "    if type(tag_score).__module__ == np.__name__:\n",
    "        return ix_to_tag.get(np.argmax(tag_score))\n",
    "    if torch.is_tensor(tag_score):\n",
    "        return ix_to_tag.get(torch.argmax(tag_score).item())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_data = [\n",
    "    (\"The dog likes the blue apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"ADJ\", \"NN\"]),\n",
    "    (\"John likes that blue book\".split(), [\"NN\", \"V\", \"DET\", \"ADJ\", \"NN\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sm = test_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['October', 'sales', ',', 'compared', '*', 'with', 'the', 'previous', 'month', ',', 'inched', 'down', '0.4', '%', '.'] ['NOUN', 'NOUN', '.', 'VERB', 'X', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'VERB', 'ADV', 'NUM', 'NOUN', '.']\n",
      "\n",
      "Mean:\n",
      "tensor([[-1.4936, -3.0441, -3.5166, -3.1182, -2.9232, -3.0208, -2.1408, -3.1277,\n",
      "         -3.5863, -3.8820, -3.0258, -3.2294],\n",
      "        [-1.5775, -2.8532, -3.2604, -3.5181, -2.7871, -3.2136, -2.2108, -3.3034,\n",
      "         -3.7129, -3.7334, -3.3108, -3.2258],\n",
      "        [-1.5602, -2.8143, -3.6457, -3.3381, -2.8239, -3.2848, -2.1105, -3.3884,\n",
      "         -3.2771, -4.0242, -3.4533, -3.2076],\n",
      "        [-1.4979, -2.6493, -3.1390, -3.4661, -2.7001, -3.4760, -2.2484, -3.8118,\n",
      "         -3.7700, -4.1569, -3.4654, -3.3058],\n",
      "        [-1.4652, -2.6129, -3.2697, -3.1860, -2.8692, -3.3591, -2.2423, -3.2747,\n",
      "         -3.7282, -3.8712, -3.3318, -3.3149],\n",
      "        [-1.4781, -2.7361, -3.0891, -3.1847, -2.3990, -3.1741, -2.2813, -3.5945,\n",
      "         -3.3305, -4.1622, -3.3531, -3.4549],\n",
      "        [-1.8560, -3.0350, -2.7556, -2.9960, -2.4866, -3.0211, -2.1607, -3.6931,\n",
      "         -3.2329, -4.0729, -3.0994, -3.1290],\n",
      "        [-1.7824, -3.0269, -3.2573, -3.4182, -2.6029, -3.0533, -2.1860, -3.9742,\n",
      "         -3.5946, -3.8165, -3.0563, -3.2025],\n",
      "        [-1.5467, -3.2121, -3.3630, -3.0169, -2.4889, -3.2422, -2.3202, -3.7559,\n",
      "         -3.5162, -3.8850, -3.6058, -3.2490],\n",
      "        [-1.6199, -3.0517, -3.4895, -3.1832, -2.7946, -3.4530, -2.2319, -3.6640,\n",
      "         -3.2765, -4.0283, -3.4534, -3.3237],\n",
      "        [-1.5166, -2.9109, -3.5063, -3.2451, -2.8136, -3.2738, -2.0665, -3.5534,\n",
      "         -3.5490, -4.2756, -3.3156, -3.0646],\n",
      "        [-1.5200, -2.6495, -3.2116, -3.3689, -2.9671, -3.4146, -1.9574, -3.3383,\n",
      "         -3.8226, -4.1255, -3.0838, -2.9613],\n",
      "        [-1.3355, -2.4809, -3.3263, -3.4621, -2.8572, -3.5233, -1.9568, -3.6650,\n",
      "         -3.7661, -4.2791, -3.2373, -2.8366],\n",
      "        [-1.6024, -2.8956, -3.6276, -3.4501, -2.7013, -3.3570, -1.9008, -3.5773,\n",
      "         -4.1254, -3.8483, -3.7256, -3.1183],\n",
      "        [-1.6332, -2.8342, -3.4710, -3.6629, -2.1896, -3.2020, -2.0714, -3.5479,\n",
      "         -3.5993, -3.4115, -3.8199, -3.2767]])\n",
      "October    NOUN\n",
      "sales      NOUN\n",
      ",          NOUN\n",
      "compared   NOUN\n",
      "*          NOUN\n",
      "with       NOUN\n",
      "the        NOUN\n",
      "previous   NOUN\n",
      "month      NOUN\n",
      ",          NOUN\n",
      "inched     NOUN\n",
      "down       NOUN\n",
      "0.4        NOUN\n",
      "%          NOUN\n",
      ".          NOUN\n",
      "\n",
      "\n",
      "Accuracy: 26.7%\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "num_samples = 10\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(test_data_sm):\n",
    "    sentence, tags = data\n",
    "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "    print(sentence, tags)\n",
    "    \n",
    "    # Convert tags into their indexes in tag dictionary\n",
    "    tag_indices = np.array([tag_to_ix.get(tag) for tag in tags])\n",
    "    \n",
    "    predicted = predict(sentence_in, num_samples)\n",
    "    total += len(tags)\n",
    "    correct += (predicted == tag_indices).sum()\n",
    "    \n",
    "    for i, token in enumerate(sentence):\n",
    "        print(f'{token:<10} {ix_to_tag.get(predicted[i])}')\n",
    "    print('\\n')\n",
    "\n",
    "print(f'Accuracy: {correct/total * 100:0.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
