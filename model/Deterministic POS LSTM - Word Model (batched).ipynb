{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilisitc POS Word Level Language Model with Penn Treebank\n",
    "Sequence Tagger: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html<br>\n",
    "Bayesian NN: https://github.com/paraschopra/bayesian-neural-network-mnist/blob/master/bnn.ipynb<br>\n",
    "Penn Treebank: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/williamFalcon/f27c7b90e34b4ba88ced042d9ef33edd <br>\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/tyler/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/tyler/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe4047e7ef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader\n",
    "Something something data loader...\n",
    "- See refs for future optimisation (perhaps, havent read): https://towardsdatascience.com/building-efficient-custom-datasets-in-pytorch-2563b946fd9f and https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennTreeBankDataset(Dataset):\n",
    "    \"\"\"Penn Tree Bank dataset\"\"\"\n",
    "    \n",
    "    def __init__(self):        \n",
    "        # load Penn tree bank sentences\n",
    "        self.sentences = treebank.tagged_sents(tagset='universal')\n",
    "        self.sentences = [self.format_sequence(sentence) for sentence in self.sentences]\n",
    "        self.len = len(self.sentences)\n",
    "        \n",
    "        self.pad_token = '<PAD>'\n",
    "        self.pad_token_id = 0\n",
    "        self.pad_len = max([len(sentence[0]) for sentence in self.sentences])\n",
    "        \n",
    "        # Vocab of input data (this will likely be a subset of any word embedding array)\n",
    "        self.build_vocab()\n",
    "        self.create_word_to_idx_dict()\n",
    "        self.create_tag_to_idx_dict()\n",
    "        self.X, self.y, self.X_lens = self.encode_and_pad_data()\n",
    "        \n",
    "    def format_sequence(self,seq):\n",
    "        \"\"\"\n",
    "        Formats penn treebank POS format into tuple ([tokens], [POS])\n",
    "        \"\"\"\n",
    "        tokens = [x[0] for x in seq]\n",
    "        tags = [x[1] for x in seq]\n",
    "        return (tokens, tags)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        \"\"\"Builds vocab dictionary based on input data\"\"\"\n",
    "        self.vocab = dict()\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence[0]:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = len(self.vocab) + 1    # counts from 1+ as 0 is reserved for PAD token\n",
    "        \n",
    "        # Add padding token to data vocab\n",
    "        self.vocab[self.pad_token] = 0\n",
    "    \n",
    "    def create_word_to_idx_dict(self):\n",
    "        \"\"\"Builds word to index dictionary\"\"\"\n",
    "        self.word_to_idx = {}\n",
    "        for sentence, tags in self.sentences:\n",
    "            for word in sentence:\n",
    "                if word not in self.word_to_idx:\n",
    "                    self.word_to_idx[word] = len(self.word_to_idx) + 1    # counts from 1+ as 0 is reserved for PAD token\n",
    "        \n",
    "        # add padding to word dict\n",
    "        self.word_to_idx[self.pad_token] = 0\n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "        \n",
    "    def create_tag_to_idx_dict(self):\n",
    "        \"\"\"Builds tag to index and index to tag dictionary\"\"\"\n",
    "        self.tag_to_idx = {}\n",
    "        for _, tags in self.sentences:\n",
    "            for tag in tags:\n",
    "                if tag not in self.tag_to_idx:\n",
    "                    self.tag_to_idx[tag] = len(self.tag_to_idx) + 1    # counts from 1+ as 0 is reserved for PAD token\n",
    "\n",
    "        # Add padding to tag dict\n",
    "        self.tag_to_idx[self.pad_token] = 0\n",
    "        self.idx_to_tag = {v:k for k, v in self.tag_to_idx.items()}\n",
    "        \n",
    "    def encode_and_pad_data(self):\n",
    "        \"\"\"Encodes data (seq and tags) into ids from id dictionary and pads\"\"\"\n",
    "        batch_size = len(self.sentences)\n",
    "\n",
    "        padded_seq_batch = np.full((batch_size, self.pad_len), self.pad_token_id)\n",
    "        padded_tags_batch = np.full((batch_size, self.pad_len), self.pad_token_id)\n",
    "        seq_lengths = [len(sentence[0]) for sentence in self.sentences]\n",
    "        \n",
    "        # encode and pad\n",
    "        for i, (seq, tags) in enumerate(self.sentences):\n",
    "            # encode\n",
    "            seq_idxs = [self.word_to_idx[token] for token in seq]\n",
    "            tags_idxs = [self.tag_to_idx[tag] for tag in tags]        \n",
    "            # pad\n",
    "            padded_seq_batch[i,0:len(seq_idxs)] = seq_idxs\n",
    "            padded_tags_batch[i, 0:len(tags_idxs)] = tags_idxs\n",
    "            \n",
    "        return torch.tensor(padded_seq_batch, dtype=torch.long), torch.tensor(padded_tags_batch, dtype=torch.long), torch.tensor(seq_lengths, dtype=torch.int)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index], self.X_lens[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PennTreeBankDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "split_ratio = 0.80\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
    "                                            [int(len(dataset)*split_ratio),\n",
    "                                             len(dataset)-int(len(dataset)*(split_ratio))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchsize 32 for training\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True,\n",
    "                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchsize 1 for test\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 3914 | Training Set Size: 3131 | Test Set Size: 783\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset Size: {len(dataset)} | Training Set Size: {len(train_dataset)} | Test Set Size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Word Embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(File,'r')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_embeddings = './data/embeddings/glove.6B.300d.txt'\n",
    "path_to_trimmed_embeddings = './data/embeddings/trimmed_emb.npz'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gloveModel = loadGloveModel(path_to_embeddings)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "emb_vocab = set(gloveModel.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://github.com/Michael-Stewart-Webdev/pytorch-lexnorm/blob/master/build_data.py\n",
    "# https://leakyrelu.com/2019/10/18/using-glove-word-embeddings-with-seq2seq-encoder-decoder-in-pytorch/\n",
    "def export_emb_vectors(data_vocab, emb_filename, trimmed_emb_filename, dim):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings = np.zeros([len(data_vocab), dim])\n",
    "\n",
    "    with codecs.open(emb_filename, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if word in data_vocab:\n",
    "                word_idx = data_vocab[word]\n",
    "                clear_output(wait=True)\n",
    "                print(word, word_idx)\n",
    "                \n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "    \n",
    "    np.savez_compressed(trimmed_emb_filename, embeddings=embeddings)\n",
    "    print('Saved trimmed embeddings to disk')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "export_emb_vectors(data_vocab, path_to_embeddings, path_to_trimmed_embeddings, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialse Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 vocab,\n",
    "                 tags,\n",
    "                 batch_size=32,\n",
    "                 pretrained_embeddings=None):\n",
    "        \n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = 1\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.tags = tags\n",
    "        self.tagset_size = len(self.tags)    # -1 not added as we want the model to predict where there is a <PAD> tag? doesn't seem right, but model works...\n",
    "#         self.tagset_size = self.tagset_size - 1 # minus <TAG>\n",
    "        self.padding_idx = self.vocab['<PAD>']\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings=self.vocab_size,\n",
    "                                            embedding_dim=self.embedding_dim,\n",
    "                                            padding_idx=self.padding_idx)\n",
    "        \n",
    "        if pretrained_embeddings is not None:\n",
    "            self.word_embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "            self.word_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # If batch_first is true the input/output tensors are provided as (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(in_features=self.hidden_dim,\n",
    "                             out_features=self.tagset_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"\n",
    "        Initialises weights for hidden layers of LSTM\n",
    "        Weights are in the form of (num_layers, batch_size, embedding_dim)\"\"\"\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim), torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\"\"\"\n",
    "        self.hidden = self.init_hidden()\n",
    "        batch_size, seq_len = X.size()       \n",
    "        \n",
    "        # Embed the input\n",
    "        # (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        embedded_seq_tensor = self.word_embeddings(X)\n",
    "        \n",
    "        # --- Run through model ---\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        # Sequences have not been sorted descending by length, this reduces the efficiency of the algorith... [TODO: fix in the data loader]\n",
    "        packed_input = torch.nn.utils.rnn.pack_padded_sequence(embedded_seq_tensor,\n",
    "                                                               [seq_len] * batch_size, # seq_lengths\n",
    "                                                               batch_first=True,\n",
    "                                                               enforce_sorted=False)\n",
    "        \n",
    "        # run through LSTM\n",
    "        packed_output, self.hidden = self.lstm(packed_input, self.hidden)\n",
    "        \n",
    "        # undo the packing operation\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # --- Project to tag space ---\n",
    "        # (batch_size, seq_len, embedding_dim) -> (batch_size * seq_len, embedding_dim)\n",
    "        output = output.contiguous()\n",
    "        output = output.view(-1, output.shape[2])\n",
    "        \n",
    "        # Run through linear layer\n",
    "        output = self.out(output)\n",
    "        \n",
    "        # Perform softmax\n",
    "        # (batch_size * seq_len, embedding_dim) -> (batch_size, seq_len, tagset_size)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "\n",
    "        # Reshape for sanity\n",
    "        output = output.view(batch_size, seq_len, self.tagset_size)   # max(seq_lengths) is for MAX across batch; max(seq_lengths)\n",
    "    \n",
    "        Y_hat = output\n",
    "        return Y_hat\n",
    "    \n",
    "        \n",
    "    def loss(self, Y_hat, Y, seq_lengths):\n",
    "        \"\"\"Compute cross-entropy loss\"\"\"\n",
    "        # Y_hat (batch_size, max_seq_len_pad, tagset_size)\n",
    "        # Y (batch_size, max_seq_len_all)      \n",
    "        \n",
    "        # ignore any words/sentences that are completely padding\n",
    "        ymask = ~torch.all(Y == 0, dim=1)\n",
    "        Y = Y[ymask]\n",
    "        Y_hat = Y_hat[ymask]\n",
    "        \n",
    "        # Truncate targets with batch max length\n",
    "        Y = Y[:, :max(seq_lengths)]\n",
    "        \n",
    "        # Flatten all the target labels\n",
    "        Y = Y.reshape(-1)   # reshape truncated targets\n",
    "        \n",
    "        # Flatten all the predicted labels\n",
    "        Y_hat = Y_hat.view(-1,self.tagset_size)\n",
    "        \n",
    "        tag_pad_token = self.tags['<PAD>']    # should be 0\n",
    "        mask = (Y > tag_pad_token).float()\n",
    "            \n",
    "        # Count how many tokens there are\n",
    "        num_tokens = int(torch.sum(mask).item())\n",
    "        \n",
    "        # Pick the values for the label and zero out the rest with the mask       \n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
    "        \n",
    "        ce_loss = -torch.sum(Y_hat) / num_tokens\n",
    "        \n",
    "        return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trimmed embeddings from disk\n",
    "pretrained_embeddings = np.load(path_to_trimmed_embeddings)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check embedding shape\n",
    "pretrained_embeddings['embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300   # Using Glove 300 dim\n",
    "HIDDEN_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_net = LSTMTagger(embedding_dim=EMBEDDING_DIM,\n",
    "                      hidden_dim=HIDDEN_DIM,\n",
    "                      vocab=dataset.vocab,\n",
    "                      tags=dataset.tag_to_idx,\n",
    "                      pretrained_embeddings=pretrained_embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(12409, 300, padding_idx=0)\n",
      "  (lstm): LSTM(300, 16, batch_first=True)\n",
      "  (out): Linear(in_features=16, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lstm_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Probabilistic Pyro Model\n",
    "Ref:<br>\n",
    "- https://forum.pyro.ai/t/bayesian-rnn-nan-loss-issue/254"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loc = mean, Scale = standard deviation\n",
    "- mu = 0, sigma = 1 -> Unit Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:<br>\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See fix for issues relating to lognorm: https://github.com/pyro-ppl/pyro/issues/1409 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input, target):\n",
    "    \n",
    "    # Embeddings (current model uses pre-trained embeddings)\n",
    "#     word_embeddings_w_prior = Normal(loc=torch.zeros_like(lstm_net.word_embeddings.weight),\n",
    "#                                      scale=torch.ones_like(lstm_net.word_embeddings.weight)).independent(2)\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_w_ih_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.weight_ih_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.weight_ih_l0)).independent(2)\n",
    "    lstm_w_hh_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.weight_hh_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.weight_hh_l0)).independent(2)\n",
    "    lstm_b_ih_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.bias_ih_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.bias_ih_l0)).independent(1)\n",
    "    lstm_b_hh_l0_prior = Normal(loc=torch.zeros_like(lstm_net.lstm.bias_hh_l0),\n",
    "                          scale=torch.ones_like(lstm_net.lstm.bias_hh_l0)).independent(1)\n",
    "    \n",
    "    # Output\n",
    "    out_w_prior = Normal(loc=torch.zeros_like(lstm_net.out.weight),\n",
    "                          scale=torch.ones_like(lstm_net.out.weight)).independent(2)\n",
    "    out_b_prior = Normal(loc=torch.zeros_like(lstm_net.out.bias),\n",
    "                          scale=torch.ones_like(lstm_net.out.bias)).independent(1)\n",
    "    \n",
    "    priors = {'lstm.weight_ih_l0': lstm_w_ih_l0_prior,\n",
    "              'lstm.weight_hh_l0': lstm_w_hh_l0_prior,\n",
    "              'lstm.bias_ih_l0': lstm_b_ih_l0_prior,\n",
    "              'lstm.bias_hh_l0': lstm_b_hh_l0_prior,\n",
    "              'out.weight': out_w_prior,\n",
    "              'out.bias': out_b_prior}   # 'word_embeddings.weight': word_embeddings_w_prior,\n",
    "    \n",
    "    # Lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", lstm_net, priors)\n",
    "    \n",
    "    # Sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "#     lhat = log_softmax(lifted_reg_model(input))\n",
    "    output = lifted_reg_model(input)\n",
    "    \n",
    "#     print(input.shape)\n",
    "#     print(target.shape)\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=output).independent(2), obs=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(input, target):\n",
    "    \n",
    "    # Embedding layer weight distribution priors (current model uses pre-trained embeddings)\n",
    "#     word_embeddings_w_mu = torch.randn_like(lstm_net.word_embeddings.weight)\n",
    "#     word_embeddings_w_sigma = torch.randn_like(lstm_net.word_embeddings.weight)\n",
    "#     word_embeddings_w_mu_param = pyro.param(\"word_embeddings_w_mu\", word_embeddings_w_mu)\n",
    "#     word_embeddings_w_sigma_param = softplus(pyro.param(\"word_embeddings_w_sigma\", word_embeddings_w_sigma))\n",
    "#     word_embeddings_w_prior = Normal(loc=word_embeddings_w_mu_param, scale=word_embeddings_w_sigma_param).independent(2)\n",
    "    \n",
    "    # LSTM layer weight distribution priors\n",
    "    lstm_w_ih_l0_mu = torch.randn_like(lstm_net.lstm.weight_ih_l0)\n",
    "    lstm_w_ih_l0_sigma = torch.randn_like(lstm_net.lstm.weight_ih_l0)\n",
    "    lstm_w_ih_l0_mu_param = pyro.param(\"lstm_w_ih_l0_mu\", lstm_w_ih_l0_mu)\n",
    "    lstm_w_ih_l0_sigma_param = softplus(pyro.param(\"lstm_w_ih_l0_sigma\", lstm_w_ih_l0_sigma))\n",
    "    lstm_w_ih_l0_prior = Normal(loc=lstm_w_ih_l0_mu_param, scale=lstm_w_ih_l0_sigma_param).independent(2)\n",
    "    \n",
    "    lstm_w_hh_l0_mu = torch.randn_like(lstm_net.lstm.weight_hh_l0)\n",
    "    lstm_w_hh_l0_sigma = torch.randn_like(lstm_net.lstm.weight_hh_l0)\n",
    "    lstm_w_hh_l0_mu_param = pyro.param(\"lstm_w_hh_l0_mu\", lstm_w_hh_l0_mu)\n",
    "    lstm_w_hh_l0_sigma_param = softplus(pyro.param(\"lstm_w_hh_l0_sigma\", lstm_w_hh_l0_sigma))\n",
    "    lstm_w_hh_l0_prior = Normal(loc=lstm_w_hh_l0_mu_param, scale=lstm_w_hh_l0_sigma_param).independent(2)\n",
    "    \n",
    "    # LSTM layer bias distribution priors\n",
    "    lstm_b_ih_l0_mu = torch.randn_like(lstm_net.lstm.bias_ih_l0)\n",
    "    lstm_b_ih_l0_sigma = torch.randn_like(lstm_net.lstm.bias_ih_l0)\n",
    "    lstm_b_ih_l0_mu_param = pyro.param(\"lstm_b_ih_l0_mu\", lstm_b_ih_l0_mu)\n",
    "    lstm_b_ih_l0_sigma_param = softplus(pyro.param(\"lstm_b_ih_l0_sigma\", lstm_b_ih_l0_sigma))\n",
    "    lstm_b_ih_l0_prior = Normal(loc=lstm_b_ih_l0_mu_param, scale=lstm_b_ih_l0_sigma_param).independent(1)\n",
    "    \n",
    "    lstm_b_hh_l0_mu = torch.randn_like(lstm_net.lstm.bias_hh_l0)\n",
    "    lstm_b_hh_l0_sigma = torch.randn_like(lstm_net.lstm.bias_hh_l0)\n",
    "    lstm_b_hh_l0_mu_param = pyro.param(\"lstm_b_hh_l0_mu\", lstm_b_hh_l0_mu)\n",
    "    lstm_b_hh_l0_sigma_param = softplus(pyro.param(\"lstm_b_hh_l0_sigma\", lstm_b_hh_l0_sigma))\n",
    "    lstm_b_hh_l0_prior = Normal(loc=lstm_b_hh_l0_mu_param, scale=lstm_b_hh_l0_sigma_param).independent(1)\n",
    "    \n",
    "    # Output layer weight distribution priors\n",
    "    out_w_mu = torch.randn_like(lstm_net.out.weight)\n",
    "    out_w_sigma = torch.randn_like(lstm_net.out.weight)\n",
    "    out_w_mu_param = pyro.param(\"out_w_mu\", out_w_mu)\n",
    "    out_w_sigma_param = softplus(pyro.param(\"out_w_sigma\", out_w_sigma))\n",
    "    out_w_prior = Normal(loc=out_w_mu_param, scale=out_w_sigma_param).independent(2)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    out_b_mu = torch.randn_like(lstm_net.out.bias)\n",
    "    out_b_sigma = torch.randn_like(lstm_net.out.bias)\n",
    "    out_b_mu_param = pyro.param(\"out_b_mu\", out_b_mu)\n",
    "    out_b_sigma_param = softplus(pyro.param(\"out_b_sigma\", out_b_sigma))\n",
    "    out_b_prior = Normal(loc=out_b_mu_param, scale=out_b_sigma_param).independent(1)\n",
    "    \n",
    "    priors = {'lstm.weight_ih_l0': lstm_w_ih_l0_prior,\n",
    "              'lstm.weight_hh_l0': lstm_w_hh_l0_prior,\n",
    "              'lstm.bias_ih_l0': lstm_b_ih_l0_prior,\n",
    "              'lstm.bias_hh_l0': lstm_b_hh_l0_prior,\n",
    "              'out.weight': out_w_prior,\n",
    "              'out.bias': out_b_prior}    # 'word_embeddings.weight': word_embeddings_w_prior,\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", lstm_net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = SVI(model,\n",
    "                guide,\n",
    "                Adam({\"lr\": 0.1}),\n",
    "                loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Probabilistic Sequence Tagger\n",
    "Training performance (10 epochs; 300/16 dim):\n",
    "- Determinisitic model | time: 33s | EOT loss: 0.7542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10\n",
      "Loss: 3463.2478\n",
      "CPU times: user 38min 58s, sys: 12.2 s, total: 39min 11s\n",
      "Wall time: 14min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(0,EPOCHS+1,1):\n",
    "    loss = 0\n",
    "    for batch_idx, (data, targets, data_lens) in enumerate(train_loader, 0):\n",
    "#         lstm_net.zero_grad()\n",
    "        \n",
    "        # Calculate loss and take gradient step\n",
    "        loss += inference.step(data, targets)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         tag_scores = lstm_net(data, data_lens)\n",
    "#         # compute loss, gradients and update parameters by calling optimzier.step()\n",
    "#         loss = lstm_net.loss(tag_scores, targets, data_lens)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f'Loss: {loss:0.4f}')\n",
    "\n",
    "    total_epoch_loss_train = loss / len(train_loader)\n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f'EPOCH: {epoch}\\nLoss: {total_epoch_loss_train:0.4f}')\n",
    "\n",
    "# print(f'Minimum Loss: {min_loss:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_F1():\n",
    "    \"\"\"\"\"\"\n",
    "    pass\n",
    "\n",
    "def loose_F1():\n",
    "    \"\"\"\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for deterministic nn inference\n",
    "def tag_score_to_tag_name(tag_score, ix_to_tag):\n",
    "    \"\"\"Converts output tag probabilities to their names\"\"\"\n",
    "    return ix_to_tag.get(torch.argmax(tag_score).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, (data, targets, data_lens) in enumerate(test_loader, 0):\n",
    "        \n",
    "        # Dimensionality: (batch_size, seq_len, tagset_size)\n",
    "        # batchsize for inference is 1\n",
    "        tag_scores = lstm_net(data, data_lens)\n",
    "        \n",
    "        # reshape tagscores, data, targets; do not need batch size dim\n",
    "        tag_scores = torch.squeeze(tag_scores)\n",
    "        data = torch.squeeze(data)\n",
    "        targets = torch.squeeze(targets)\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            for i in range(0, tag_scores.shape[0]):   # iterate over preds from tag_scores (minimal padding rather than data/targets with max seq padding)\n",
    "                token = dataset.idx_to_word[data[i].item()]   # detatch tensor, extract token idx\n",
    "                target = dataset.idx_to_tag[targets[i].item()]\n",
    "                \n",
    "                print(f'{token:<20} {tag_score_to_tag_name(tag_scores[i], dataset.idx_to_tag):<10} {target:<10}')\n",
    "        \n",
    "        # compute accuracy\n",
    "        # Implement accuracy...; F1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(test_data_sm[0][0], word_to_ix)\n",
    "    tag_scores = lstm_net(inputs)\n",
    "    \n",
    "#     print(f'Tag Scores:\\n{tag_scores}\\n')\n",
    "    print(f'{\"Token\":<20} {\"Pred\":<10} {\"Actual\":<10}')\n",
    "    print(f'{\"-----\":<20} {\"----\":<10} {\"------\":<10}')\n",
    "    for i, token in enumerate(training_data[0][0]):\n",
    "        print(f'{token:<20} {tag_score_to_tag_name(tag_scores[i], ix_to_tag):<10} {test_data_sm[0][1][i]:<10}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
